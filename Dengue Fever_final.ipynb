{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc996f1-a805-4603-a384-5409575684d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "#Removing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d5832d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'submission_format.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nick/Desktop/Dengue_Fever/Dengue Fever_final.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nick/Desktop/Dengue_Fever/Dengue%20Fever_final.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdengue_features_test.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nick/Desktop/Dengue_Fever/Dengue%20Fever_final.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Load the submission dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nick/Desktop/Dengue_Fever/Dengue%20Fever_final.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test_submission \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39msubmission_format.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nick/Desktop/Dengue_Fever/Dengue%20Fever_final.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Removing limits on the number of columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nick/Desktop/Dengue_Fever/Dengue%20Fever_final.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.max_columns\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'submission_format.csv'"
     ]
    }
   ],
   "source": [
    "# Load the training features dataset\n",
    "train_features = pd.read_csv('dengue_features_train.csv')\n",
    "\n",
    "# Load the training labels dataset\n",
    "train_labels = pd.read_csv('dengue_labels_train.csv')\n",
    "\n",
    "# Load the test features dataset\n",
    "test_features = pd.read_csv('dengue_features_test.csv')\n",
    "\n",
    "# Load the submission dataset\n",
    "test_submission = pd.read_csv('submission_format.csv')\n",
    "\n",
    "# Removing limits on the number of columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87084dc1-7604-4e5a-b921-2da4fb5536ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape,train_labels.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea12d5-749e-4aef-88bb-f11d74753e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the train_features dataset\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78bb0ca-844f-4e86-b050-bf877a3bf112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the train_labels dataset\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebb36c-0817-4763-b5af-facc096bb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the test_features dataset\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219547f-db79-4aad-a637-790dde388871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the train_feature dataset\n",
    "missing_data = train_features.isnull().sum()\n",
    "missing_columns = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "missing_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed634251-2a10-4cd5-b82f-5b23eab4f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before deciding on imputation strategies, \n",
    "#let's also check for missing values in the test_features dataset to ensure consistency in our approach across both datasets\n",
    "missing_data = test_features.isnull().sum()\n",
    "missing_columns = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "missing_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43f259-1824-4319-81d5-843acf100a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the columns into both numerical and categorical\n",
    "\n",
    "# Identify columns with missing values in train_features\n",
    "missing_data_train = train_features.isnull().sum()\n",
    "missing_columns_train = missing_data_train[missing_data_train > 0].index\n",
    "\n",
    "# Categorize the missing columns into numerical and categorical types\n",
    "numerical_columns = [col for col in missing_columns_train if train_features[col].dtype in ['int64', 'float64']]\n",
    "categorical_columns = [col for col in missing_columns_train if train_features[col].dtype == 'object']\n",
    "\n",
    "numerical_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd272aba-1050-48b6-80da-0f726b295ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the columns into both numerical and categorical\n",
    "\n",
    "# Identify columns with missing values in train_features\n",
    "missing_data_test = test_features.isnull().sum()\n",
    "missing_columns_test = missing_data_train[missing_data_train > 0].index\n",
    "\n",
    "# Categorize the missing columns into numerical and categorical types\n",
    "numerical_columns = [col for col in missing_columns_train if test_features[col].dtype in ['int64', 'float64']]\n",
    "categorical_columns = [col for col in missing_columns_train if test_features[col].dtype == 'object']\n",
    "\n",
    "numerical_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba70de7-62cf-499a-843c-24e94e9e4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We don't have any categorical values. We shall proceed with imputation strategies for the numerical columns\n",
    "\n",
    "#1. We shall interpolate ndvi columns:\n",
    "ndvi_columns = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw']\n",
    "train_features[ndvi_columns] = train_features[ndvi_columns].interpolate()\n",
    "\n",
    "#2. We shall impute the temperature and humidity-related columns using the median\n",
    "temp_humidity_cols = ['station_avg_temp_c', 'reanalysis_air_temp_k', 'reanalysis_avg_temp_k',\n",
    "                          'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k',\n",
    "                          'reanalysis_relative_humidity_percent', 'station_max_temp_c', 'station_min_temp_c',\n",
    "                          'station_diur_temp_rng_c', 'reanalysis_tdtr_k', 'reanalysis_specific_humidity_g_per_kg']\n",
    "for col in temp_humidity_cols:\n",
    "    median_value = train_features[col].median()\n",
    "    train_features[col].fillna(median_value, inplace=True)\n",
    "\n",
    "#3.Fill missing values in precipitation columns with 0:\n",
    "precipitation_cols = ['station_precip_mm', 'reanalysis_sat_precip_amt_mm', 'reanalysis_precip_amt_kg_per_m2','precipitation_amt_mm']\n",
    "train_features[precipitation_cols] = train_features[precipitation_cols].fillna(0)\n",
    "\n",
    "#4. Checking if we have any missing values now:\n",
    "missing_after_imputation = train_features.isnull().sum().sum()\n",
    "print(missing_after_imputation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3452a4-489f-4d7c-b7d1-6efe86de641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We don't have any categorical values. We shall proceed with imputation strategies for the numerical columns\n",
    "\n",
    "#1. We shall interpolate ndvi columns:\n",
    "ndvi_columns = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw']\n",
    "test_features[ndvi_columns] = test_features[ndvi_columns].interpolate()\n",
    "\n",
    "#2. We shall impute the temperature and humidity-related columns using the median\n",
    "temp_humidity_cols = ['station_avg_temp_c', 'reanalysis_air_temp_k', 'reanalysis_avg_temp_k',\n",
    "                          'reanalysis_dew_point_temp_k', 'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k',\n",
    "                          'reanalysis_relative_humidity_percent', 'station_max_temp_c', 'station_min_temp_c',\n",
    "                          'station_diur_temp_rng_c', 'reanalysis_tdtr_k', 'reanalysis_specific_humidity_g_per_kg']\n",
    "for col in temp_humidity_cols:\n",
    "    median_value = test_features[col].median()\n",
    "    test_features[col].fillna(median_value, inplace=True)\n",
    "\n",
    "#3.Fill missing values in precipitation columns with 0:\n",
    "precipitation_cols = ['station_precip_mm', 'reanalysis_sat_precip_amt_mm', 'reanalysis_precip_amt_kg_per_m2','precipitation_amt_mm']\n",
    "test_features[precipitation_cols] = test_features[precipitation_cols].fillna(0)\n",
    "\n",
    "#4. Checking if we have any missing values now:\n",
    "missing_after_imputation = test_features.isnull().sum().sum()\n",
    "print(missing_after_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8664c1-7238-456f-ac88-7128103015a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are now done with missing values.Let's merge train_features with train_labels on ['city', 'year', 'weekofyear']\n",
    "train_data = pd.merge(train_features, train_labels, on=['city', 'year', 'weekofyear'])\n",
    "\n",
    "# Check the first few rows of the merged dataset\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54558bd6-37ac-4c6c-b3e4-adddcd0c1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.city.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845c580-35e9-4377-9306-47d63fc1f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape,test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a5336-2afe-480c-b407-9838d3aee32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Plot\n",
    "\n",
    "train_data['approx_date'] = train_data['year'].astype(str) + '.' + train_data['weekofyear'].astype(str)\n",
    "\n",
    "# Set the interval for showing x-ticks. We'll show every 50th tick for clarity.\n",
    "interval = 50\n",
    "\n",
    "# Plotting total_cases for each city\n",
    "plt.figure(figsize=(20, 8))  # Increased the figure size\n",
    "for city in ['sj', 'iq']:\n",
    "    city_data = train_data[train_data['city'] == city]\n",
    "    plt.plot(city_data['approx_date'], city_data['total_cases'], label=city)\n",
    "\n",
    "# Customizing the plot\n",
    "plt.title('Dengue Fever Cases Over Time')\n",
    "plt.xlabel('Time (Year.WeekofYear)')\n",
    "plt.ylabel('Total Cases')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=90)  # Increased rotation for x-ticks\n",
    "\n",
    "# Adjusting x-ticks for better visibility\n",
    "ticks = plt.gca().get_xticks()\n",
    "plt.gca().set_xticks(ticks[::interval])  # Show every 50th tick\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3aeba-26ce-4fba-99fd-014fe4a4d6b9",
   "metadata": {},
   "source": [
    "Seasonality: Both cities show a clear seasonality in dengue cases, with peaks and troughs occurring at regular intervals.\n",
    "\n",
    "Trend: San Juan (sj) seems to have a more noticeable upward trend in cases in the earlier years, which stabilizes later on. Iquitos (iq), on the other hand, has a relatively consistent number of cases across years, with some occasional spikes.\n",
    "\n",
    "Differences Between Cities: San Juan generally has a higher number of cases compared to Iquitos. The patterns of dengue cases also differ between the two cities. This shows  the importance of considering city-specific factors when modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d631b3-8b4e-48a5-ac02-1f4cc04ef69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "\n",
    "# Exclude non-numeric columns and compute the correlation matrix\n",
    "numeric_data = train_data.select_dtypes(include=['float64', 'int64'])\n",
    "correlation_matrix_numeric = numeric_data.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "# Generate a heatmap for the numeric data\n",
    "sns.heatmap(correlation_matrix_numeric, annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.title('Correlation Heatmap for Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89769a-80ad-4ec3-b26f-d697ded28335",
   "metadata": {},
   "source": [
    "Positive Correlations with total_cases:\n",
    "\n",
    "Some temperature and humidity metrics seem to have a moderate positive correlation with total_cases.\n",
    "\n",
    "\n",
    "Negative Correlations with total_cases:\n",
    "\n",
    "While there might be fewer strongly negative correlations in this dataset, it's essential to recognize any feature that might act inversely to dengue cases.\n",
    "\n",
    "High Inter-feature Correlations:\n",
    "\n",
    "Some features might be highly correlated with each other, indicating potential multicollinearity. This may impact the interpretability and stability of linear regression coefficients.\n",
    "For instance, various temperature measurements seem to be correlated with each other. The same goes for some humidity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b51f9-d071-4d39-a916-8a0dabebc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features we want to visualize using boxplots\n",
    "selected_features = ['reanalysis_air_temp_k', 'reanalysis_avg_temp_k', 'reanalysis_dew_point_temp_k',\n",
    "                     'reanalysis_max_air_temp_k', 'reanalysis_min_air_temp_k', 'reanalysis_relative_humidity_percent',\n",
    "                     'station_avg_temp_c', 'station_max_temp_c', 'station_min_temp_c']\n",
    "\n",
    "# Plot boxplots for each of the selected features, separated by city\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for index, feature in enumerate(selected_features, 1):\n",
    "    plt.subplot(3, 3, index)\n",
    "    sns.boxplot(data=train_data, x='city', y=feature)\n",
    "    plt.title(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d92bf-67f4-4102-8b20-33bd0d1866a0",
   "metadata": {},
   "source": [
    "\n",
    "The box plots show that there are outliers that have to be dealt with\n",
    "\n",
    "Feature Differences by City: Some features, like reanalysis_air_temp_k and reanalysis_avg_temp_k, show noticeable differences in their distributions between the two cities. This reinforces the idea that city-specific factors can significantly impact the data and modeling strategies should account for these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8bcf9-45ec-4589-8ac1-170e38321291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms for selected features to understand their distributions\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for index, feature in enumerate(selected_features, 1):\n",
    "    plt.subplot(3, 3, index)\n",
    "    sns.histplot(train_data[feature], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c38d5b-7d9a-4335-9512-d7fbaaa33d2d",
   "metadata": {},
   "source": [
    "\n",
    "Variability: The spread of the data in the histograms provides insights into the variability of each feature.\n",
    "\n",
    "Peaks: Multiple peaks in a distribution might suggest multiple sub-groups within the data or bimodal distributions.\n",
    "\n",
    "Skewness: Some distributions show a bit of skewness either to the right (positive skew) or to the left (negative skew). Skewed distributions might sometimes benefit from transformations (like logarithmic or square root transformations) to make them more symmetric, especially for linear models that assume normally distributed errors.\n",
    "\n",
    "KDE Overlay: The smooth line (Kernel Density Estimation) gives a smoothed version of the histogram and can help in identifying the distribution's mode(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe59c67-f451-418b-a2ad-89deefdb6412",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca35d0f-00b3-4f9a-94f2-022f4310037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Time-based Features. I'm trying to decide whether to convert the weeks  into months or quarters\n",
    "\n",
    "# Extracting month from the week_start_date\n",
    "train_data['month'] = pd.to_datetime(train_data['week_start_date']).dt.month\n",
    "\n",
    "# Grouping by month and calculating the average number of cases\n",
    "average_cases_by_month = train_data.groupby('month')['total_cases'].mean()\n",
    "\n",
    "# Grouping by quarter and calculating the average number of cases\n",
    "average_cases_by_quarter = train_data.groupby(train_data['month'] // 4 + 1)['total_cases'].mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Monthly average cases\n",
    "plt.subplot(1, 2, 1)\n",
    "average_cases_by_month.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Dengue Cases by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Cases')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Quarterly average cases\n",
    "plt.subplot(1, 2, 2)\n",
    "average_cases_by_quarter.plot(kind='bar', color='salmon')\n",
    "plt.title('Average Dengue Cases by Quarter')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Average Cases')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e812e-d409-4a80-9afc-084f861c26ce",
   "metadata": {},
   "source": [
    "Months show finer distinctions. So I shall use months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb71f32-d8a9-4646-8a2b-5337b1e13960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the month variables\n",
    "\n",
    "# Convert month to a categorical variable\n",
    "train_data['month'] = train_data['month'].astype('category')\n",
    "\n",
    "# One-hot encode the month column\n",
    "train_data = pd.get_dummies(train_data, columns=['month'], prefix='month', drop_first=True)\n",
    "\n",
    "# Display the first few rows to show the one-hot encoded month columns\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52333021-60fb-4bb9-b67d-d6d7d5e6461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the month variables\n",
    "test_features['month'] = pd.to_datetime(test_features['week_start_date']).dt.month\n",
    "# Convert month to a categorical variable\n",
    "test_features['month'] = test_features['month'].astype('category')\n",
    "\n",
    "# One-hot encode the month column\n",
    "test_features = pd.get_dummies(test_features, columns=['month'], prefix='month', drop_first=True)\n",
    "\n",
    "# Display the first few rows to show the one-hot encoded month columns\n",
    "test_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17acf1-2992-4915-ada9-dd186528527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating interaction features\n",
    "train_data['temp_precip_interaction'] = train_data['reanalysis_avg_temp_k'] * train_data['reanalysis_precip_amt_kg_per_m2']\n",
    "train_data['temp_range_interaction'] = train_data['reanalysis_min_air_temp_k'] * train_data['reanalysis_max_air_temp_k']\n",
    "train_data['dew_temp_interaction'] = train_data['reanalysis_dew_point_temp_k'] * train_data['reanalysis_avg_temp_k']\n",
    "\n",
    "# Displaying the first few rows with the new interaction features\n",
    "train_data[['reanalysis_avg_temp_k', 'reanalysis_precip_amt_kg_per_m2', 'temp_precip_interaction',\n",
    "            'reanalysis_min_air_temp_k', 'reanalysis_max_air_temp_k', 'temp_range_interaction',\n",
    "            'reanalysis_dew_point_temp_k', 'reanalysis_avg_temp_k', 'dew_temp_interaction']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f6af2-a25d-47ef-843f-3cb522b3dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating interaction features\n",
    "test_features['temp_precip_interaction'] = test_features['reanalysis_avg_temp_k'] * test_features['reanalysis_precip_amt_kg_per_m2']\n",
    "test_features['temp_range_interaction'] = test_features['reanalysis_min_air_temp_k'] * test_features['reanalysis_max_air_temp_k']\n",
    "test_features['dew_temp_interaction'] = test_features['reanalysis_dew_point_temp_k'] * test_features['reanalysis_avg_temp_k']\n",
    "\n",
    "# Displaying the first few rows with the new interaction features\n",
    "test_features[['reanalysis_avg_temp_k', 'reanalysis_precip_amt_kg_per_m2', 'temp_precip_interaction',\n",
    "            'reanalysis_min_air_temp_k', 'reanalysis_max_air_temp_k', 'temp_range_interaction',\n",
    "            'reanalysis_dew_point_temp_k', 'reanalysis_avg_temp_k', 'dew_temp_interaction']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd85e7a-34cd-488c-b3f3-8f2f6ef8b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity_cols.extend(['temp_precip_interaction','temp_range_interaction','dew_temp_interaction'])\n",
    "\n",
    "# Creating 4-week lags for the temp_humidity_cols in train dataset\n",
    "for column in temp_humidity_cols:\n",
    "    train_data[f'{column}_lag4'] = train_data.groupby('city')[column].shift(4)\n",
    "\n",
    "# Display the first few rows with the new lagged features\n",
    "train_data[['city', 'year', 'weekofyear'] + [f'{column}_lag4' for column in temp_humidity_cols ]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d90ab-308a-4412-8b49-4af75f651099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 4-week lags for the temp_humidity_cols in the test dataset\n",
    "for column in temp_humidity_cols:\n",
    "    test_features[f'{column}_lag4'] = test_features.groupby('city')[column].shift(4)\n",
    "\n",
    "# Display the first few rows with the new lagged features\n",
    "test_features[['city', 'year', 'weekofyear'] + [f'{column}_lag4' for column in temp_humidity_cols]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647af194-662f-4a6d-9bdf-8d17406196dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle outliers using the IQR method\n",
    "def handle_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Capping the outliers\n",
    "    data[column] = data[column].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Handling outliers for the selected features\n",
    "for feature in selected_features:\n",
    "    train_data = handle_outliers_iqr(train_data, feature)\n",
    "\n",
    "# Displaying the cleaned data\n",
    "train_data[selected_features].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85a0ac-ffd1-4d82-ba91-2a51f95394f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle outliers using the IQR method\n",
    "def handle_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Capping the outliers\n",
    "    data[column] = data[column].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Handling outliers for the selected features\n",
    "for feature in selected_features:\n",
    "   test_features = handle_outliers_iqr(test_features, feature)\n",
    "\n",
    "# Displaying the cleaned data\n",
    "test_features[selected_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32e4ed-0951-4fae-bc87-36ed6b5707d4",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048daa07-6edb-490f-b2bb-f09ece113fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to exclude non-numerical columns\n",
    "numerical_data = train_data.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calculate the correlation of each numerical feature with the target variable 'total_cases'\n",
    "numerical_correlations = numerical_data.corr()['total_cases'].sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations\n",
    "numerical_correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbae4c8-4e49-4cf2-b5fb-51520f4c2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features with an absolute correlation value greater than 0.1\n",
    "selected_features = numerical_correlations[abs(numerical_correlations) > 0.1].index.tolist()\n",
    "\n",
    "# Excluding the target variable 'total_cases' from the selected features list\n",
    "selected_features.remove('total_cases')\n",
    "\n",
    "# Displaying the selected features\n",
    "selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d1541-abad-42cf-a1d9-a7ecdf45884c",
   "metadata": {},
   "source": [
    "Feature selection by Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1730f4d-2799-4b83-90a2-4a3ab90c9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the rows with NaN values in the lagged columns\n",
    "nan_values = train_data.isnull().sum()\n",
    "\n",
    "nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d46144-17a6-4cc1-89ee-b0df778dee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed1862-9e14-4dae-aa32-a1d5e3e3c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b38ad-1787-4f4c-8ada-2c4c49d35757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the columns list to drop and then proceeding\n",
    "columns_to_drop = ['city', 'year', 'weekofyear', 'week_start_date', 'total_cases']\n",
    "\n",
    "X = train_data.drop(columns_to_drop, axis=1)\n",
    "y = train_data['total_cases']\n",
    "\n",
    "# Initializing the random forest regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# Training the random forest regressor\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Extracting feature importances and creating a DataFrame for visualization\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Displaying the top 20 features\n",
    "top_features = feature_importances.head(25)\n",
    "\n",
    "top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d5c49-b29a-4926-90ea-31cb752812a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the intersection of the top features from both the correlation method and the Random Forest feature importance.\n",
    "top_features_list = top_features['feature'].tolist()\n",
    "\n",
    "common_features = [feature for feature in selected_features if feature in top_features_list]\n",
    "common_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687ec6f-9d79-42e7-af30-de78d6e1d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the top features for both the train_data and test_features\n",
    "\n",
    "train_data = train_data[common_features + ['total_cases', 'city', 'year', 'weekofyear']]\n",
    "test_features = test_features[common_features + ['city', 'year', 'weekofyear']]\n",
    "\n",
    "print(train_data.shape, test_features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24cf4a-63c7-406a-b628-5c8dd35d8b2b",
   "metadata": {},
   "source": [
    "**Split the data into training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b651f96-1c2b-4988-b452-55acbfbd4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Create the validation sets\n",
    "X = train_data.drop(columns=['total_cases'])\n",
    "y = train_data['total_cases']\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082a3b7-ca15-4c38-86ac-2ea95c3958c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding the 'city' variable for both training and validation sets\n",
    "X_train = pd.get_dummies(X_train, columns=['city'], drop_first=True)\n",
    "X_val = pd.get_dummies(X_val, columns=['city'], drop_first=True)\n",
    "\n",
    "X_train.head()\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e69c2-16e3-41ac-9a67-d39c16436abf",
   "metadata": {},
   "source": [
    "**Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d7d5e-e28e-44b0-bdf6-fd4bff89f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the training data using the robust scaler\n",
    "X_train_scaled = robust_scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data using the robust scaler\n",
    "X_val_scaled = robust_scaler.transform(X_val)\n",
    "\n",
    "X_train_scaled[:5]  # Display the first 5 rows of the scaled training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c422e1-6e8a-418a-b196-65e874a042fe",
   "metadata": {},
   "source": [
    "**Feature Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c17cd-cacc-41f1-94e0-a08a474d5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize the linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Train the model using the scaled training data\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = lin_reg.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the mean absolute error\n",
    "mae_linear_regression = mean_absolute_error(y_val, y_val_pred)\n",
    "mae_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f057b4-5296-4e55-98ee-e75cd9dcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha parameter for regularization strength\n",
    "\n",
    "# Train the model\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_ridge = ridge_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the mean absolute error\n",
    "mae_ridge = mean_absolute_error(y_val, y_pred_ridge)\n",
    "print(f\"Mean Absolute Error (Ridge Regression): {mae_ridge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f453503-f150-4fa7-bac8-d50d957ba016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Decision Tree:\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "tree_model = DecisionTreeRegressor(max_depth=10)  # Adjust max_depth for tree depth\n",
    "\n",
    "# Train the model\n",
    "tree_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_tree = tree_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the mean absolute error\n",
    "mae_tree = mean_absolute_error(y_val, y_pred_tree)\n",
    "print(f\"Mean Absolute Error (Decision Trees): {mae_tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b28b88-8da7-4347-a927-fab6ac1accd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_rf = rf_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the mean absolute error\n",
    "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
    "print(f\"Mean Absolute Error (Random Forest): {mae_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cd08e-6034-4088-9ad4-4a714b278b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter Optimization for Gradient Boosting Regressor\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameters and their possible values\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Set up GridSearchCV for Gradient Boosting\n",
    "grid_search_gb = GridSearchCV(gb, param_grid_gb, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit to the data\n",
    "grid_search_gb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_params_gb = grid_search_gb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef069486-c82e-4f66-a30c-18ee5e5178e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters from the grid search for Gradient Boosting\n",
    "best_gb_params = grid_search_gb.best_params_\n",
    "\n",
    "# Initialize the Gradient Boosting model with the best hyperparameters\n",
    "gb_model = GradientBoostingRegressor(**best_gb_params, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_gb = gb_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the mean absolute error\n",
    "mae_gb = mean_absolute_error(y_val, y_pred_gb)\n",
    "print(f\"Mean Absolute Error (Gradient Boosting): {mae_gb}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfc5db-cc57-4d68-a377-be0d29d5fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Hyper parameter Optimizationfor XGBoost\n",
    "\n",
    "xgb_regressor = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameters and their possible values\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'max_depth': [5, 6, 7]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for XGBoost\n",
    "grid_search_xgb = GridSearchCV(xgb_regressor, param_grid_xgb, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit to the data\n",
    "grid_search_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_params_xgb = grid_search_xgb.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1279b-f826-4fe4-9042-a62b51619880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using the best hyperparameters from the grid search for XGBoost\n",
    "best_xgb_params = grid_search_xgb.best_params_\n",
    "\n",
    "# Initialize the XGBRegressor with the best hyperparameters\n",
    "xgb_regressor = xgb.XGBRegressor(**best_xgb_params, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_xgb = xgb_regressor.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the mean absolute error for XGBoost\n",
    "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
    "print(f\"Mean Absolute Error (XGBoost): {mae_xgb}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8df62b-d24a-4735-9c55-b9245647d1bf",
   "metadata": {},
   "source": [
    "We will use a **Stacked Model of Xgboost and Gradient Boosting** as they both have the **lowest MAE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c01ba-1179-4ea9-9969-d21d1ce85554",
   "metadata": {},
   "source": [
    "**Feature Engineering the Test Data and creating Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540335d-6d9e-485f-a09f-3634af69eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handling NaN values in the test features\n",
    "nan_columns = test_features.columns[test_features.isnull().any()]\n",
    "for col in nan_columns:\n",
    "    median_value = train_data[col].median()  # Using median from training data to avoid data leakage\n",
    "    test_features[col].fillna(median_value, inplace=True)\n",
    "\n",
    "# One-hot encode city column and dropping total_cases \n",
    "test_features = pd.get_dummies(test_features, columns=['city'], drop_first=True) \n",
    "test_features = test_features.drop('total_cases', axis=1, errors='ignore') \n",
    "\n",
    "# Scaling test features\n",
    "test_scaled = robust_scaler.transform(test_features)\n",
    "\n",
    "# Predicting on the validation set using Gradient Boosting and XGBoost\n",
    "y_pred_gb = gb_model.predict(X_val_scaled)\n",
    "y_pred_xgb = xgb_regressor.predict(X_val_scaled)\n",
    "\n",
    "# Stacking the predictions from the base models to form a new training set for the meta-model\n",
    "stacked_predictions = np.column_stack((y_pred_gb, y_pred_xgb))\n",
    "\n",
    "# Initializing and training the meta-model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_predictions, y_val)\n",
    "\n",
    "# Predicting using individual models on the test set\n",
    "gb_test_preds = gb_model.predict(test_scaled)\n",
    "xgb_test_preds = xgb_regressor.predict(test_scaled)\n",
    "\n",
    "# Stacking the test set predictions\n",
    "stacked_test_predictions = np.column_stack((gb_test_preds, xgb_test_preds))\n",
    "\n",
    "# Use the meta-model to make final predictions on the test set\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "final_predictions = final_predictions.round().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb6a25-18c4-45b5-b9a7-6bc1f7d737a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_features['city'] = np.where(test_features['city_sj'] == 1, 'sj', 'iq')\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"city\": test_features.city,\n",
    "        \"year\": test_features.year,\n",
    "    \"weekofyear\":test_features.weekofyear,\n",
    "    \"total_cases\":final_predictions\n",
    "    })\n",
    "submission\n",
    "submission.to_csv('submission_stacked.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e78cf-5863-459b-976a-e31f94c501fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825464c9-16ca-4772-a5f1-071b8500f0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
